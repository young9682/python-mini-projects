### 代码核心功能说明
1. **模块化设计**：
   - `get_chapter_content()`：负责爬取单章内容、保存、获取下一章链接
   - `crawl_full_book()`：主函数，处理用户输入、创建文件夹、循环爬取
2. **连续爬取核心逻辑**：
   - 通过XPath `//a[contains(text(), "下一章") or contains(@class, "next")]/@href` 匹配下一章链接（适配大多数小说网站）
   - 自动处理相对路径（通过`urljoin`转换为完整URL）
   - 循环爬取直到没有下一章链接
3. **文件保存优化**：
   - 创建以书籍名称命名的文件夹
   - 所有章节内容合并保存到 `full_book.txt` 文件中（用`=== 章节名 ===`分隔各章）
   - 追加写入（`a`模式），避免覆盖已爬取内容
4. **防封IP/稳定性优化**：
   - 添加1秒爬取间隔（`time.sleep(delay)`），可根据需要调整
   - 增加`Referer`请求头，模拟真实浏览器访问
   - 捕获`KeyboardInterrupt`，支持用户按`Ctrl+C`安全终止
   - 详细的日志提示（✅/❌/⚠️），方便查看爬取状态

### 使用前注意事项
1. 安装依赖（如果未安装）：
   ```bash
   pip install requests lxml
2. 适配不同网站的调整点（关键）：
   正文 XPath：如果爬取的网站正文不是//div[@class="content"]/text()，需要修改get_chapter_content中的content_list对应的 XPath
   - 下一章 XPath：如果网站下一章按钮的文字 / 类名不是 “下一章”/“next”，需要修改next_chapter_links对应的 XPath
   - 标题 XPath：如果标题不是//h1/text()，修改title_list对应的 XPath
3. 爬取间隔：如果网站反爬严格，可将delay从 1 秒调整为 2-3 秒  

### 总结
1. 核心新增功能：通过匹配 “下一章” 链接实现全自动连续爬取整本书，所有章节合并保存到一个文件。
2. 适配性优化：处理相对路径、自定义书籍名称、适配常见小说网站的 XPath 规则。
3. 稳定性保障：添加延时、异常捕获、用户终止处理，降低被封 IP 和程序崩溃的风险。
4. 灵活调整：可根据目标网站的 HTML 结构，修改 XPath 路径适配不同网站。  

*** 你只需要输入小说第一章的 URL、User-Agent 和书籍名称，脚本就会自动爬取所有章节并保存，直到没有下一章为止。如果爬取过程中某一章失败，会提示错误但继续爬取下一章，不会整体中断。***

*****  

# Novel Worm - 小说全本爬虫（增强稳定版 v2.1）
一款高适配、高稳定的小说全本自动爬取工具，支持多网站适配、广告内容过滤、防循环爬取，可自动连续爬取整本书并保存为结构化 TXT 文件。

## 代码核心功能说明
1. **模块化设计**：
   - `clean_text()`：清理正文中的广告、导航栏、水印等无关内容，保留纯小说文本
   - `extract_content()`：智能匹配多套 XPath 规则提取正文，适配不同小说网站
   - `find_next_chapter_url()`：多策略查找下一章链接，支持关键词/分页栏/标准 rel="next" 匹配
   - `get_chapter_content()`：爬取单章内容、重试失败请求、写入文件、返回下一章链接
   - `crawl_full_book()`：主函数，处理用户输入校验、创建存储目录、循环爬取整本书
2. **智能爬取核心逻辑**：
   - 正文提取：内置 10 套高频正文容器 XPath 规则，按优先级自动尝试匹配（content、txt、read-content 等）
   - 下一章匹配：支持 8 种关键词（下一章/Next/→等）+ 分页栏最后一个链接 + 标准 rel="next" 多策略匹配
   - 路径处理：自动将相对路径转换为完整 URL，验证 URL 有效性（含 scheme/netloc）
   - 防循环：记录已访问 URL，检测到循环链接自动停止爬取
3. **文件保存优化**：
   - 自动创建以书籍名称命名的文件夹（自动处理非法字符）
   - 所有章节合并保存为 `{书籍名}.txt`，章节格式为 `==== 第 X 章：标题 ====`，段落分隔清晰
   - 实时刷新文件写入（flush），避免程序中断导致内容丢失
   - 自动截断过长标题（≤60字符），防止文件命名异常
4. **防封IP/稳定性优化**：
   - 1.5秒爬取间隔（可调整），重试间隔递增（2s→4s），降低被反爬识别概率
   - 完善的请求头配置（User-Agent/Referer/Accept-Language 等），模拟真实浏览器访问
   - 自动编码检测（apparent_encoding），解决乱码问题
   - 多维度异常捕获：请求超时/解析错误/用户中断/循环链接，保证程序不崩溃
   - 详细的状态提示（✅/⚠️/❌/🔄），直观展示爬取进度和异常
5. **内容净化优化**：
   - 过滤 15+ 类广告/干扰文本（域名提示、手机阅读、打赏、分页导航等）
   - 保留原始段落结构，符合小说阅读习惯
   - 内容长度校验（≥50字符），过滤无效匹配

## 使用前注意事项
1. 安装依赖（如果未安装）：
   ```bash
   pip install requests lxml

   