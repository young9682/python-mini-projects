import requests
from lxml import etree
import time
import os
import re
from urllib.parse import urljoin, urlparse

# ===== é…ç½®åŒº =====
CONTENT_XPATHS = [
    '//div[@id="content"]',
    '//div[@class="content"]',
    '//div[@id="txt"]',
    '//div[@class="txt"]',
    '//div[contains(@class, "read-content")]',
    '//div[contains(@class, "article")]',
    '//div[@class="panel-body"]',
    '//div[@class="page-content"]',
    '//div[@id="chapter_content"]',
    '//div[contains(@class, "chapter") and contains(@class, "content")]',
]

NEXT_CHAPTER_KEYWORDS = ["ä¸‹ä¸€ç« ", "ä¸‹ä¸€é¡µ", "Next Chapter", "Next", "â†’", "Â»", "ä¸‹èŠ‚", "ç»§ç»­é˜…è¯»"]

FILTER_PATTERNS = [
    r'è¯·è®°ä½æœ¬ç«™åŸŸå',
    r'æ‰‹æœºé˜…è¯».*?ç½‘å€',
    r'æœ€æ–°ç« èŠ‚.*?é¦–å‘',
    r'å…è´¹é˜…è¯».*?å…¨æ–‡',
    r'ç‚¹å‡»è¿›å…¥.*?é˜…è¯»æœ€æ–°ç« èŠ‚',
    r'æœ¬ä¹¦æ¥è‡ª.*?é˜…è¯»ç½‘',
    r'æ›´å¤šå…è´¹å°è¯´.*?ä¸‹è½½',
    r'æ‰«æäºŒç»´ç .*?æ‰‹æœºé˜…è¯»',
    r'æŠ•ç¥¨æ¨è\s*åŠ å…¥ä¹¦ç­¾\s*ç•™è¨€åé¦ˆ',
    r'www\.[a-zA-Z0-9\-]+\.(com|net|org|cc|me)',
    r'http[s]?://[^\s]+',
    r'æœ¬ç« æœªå®Œ.*?è¯·ç‚¹å‡»ä¸‹ä¸€é¡µç»§ç»­é˜…è¯»',
    r'åŠ å…¥ä¹¦æ¶|æ”¶è—æœ¬ç«™|æ‰“èµä½œè€…|æ¨èç¥¨',
    r'ç¬¬\d+é¡µ/å…±\d+é¡µ',
    r'ã€.*?ã€æ›´æ–°æœ€å¿«',  # å¹¿å‘Šæ°´å°
    r'é¦–é¡µ\s*ä¸Šä¸€ç« \s*ä¸‹ä¸€ç« \s*æœ«é¡µ',  # åˆ†é¡µå¯¼èˆª
]

# ===== å·¥å…·å‡½æ•° =====

def clean_text(text):
    lines = text.split('\n')
    cleaned = []
    for line in lines:
        stripped = line.strip()
        if not stripped:
            continue
        if any(re.search(pat, stripped, re.IGNORECASE) for pat in FILTER_PATTERNS):
            continue
        cleaned.append(stripped)
    return '\n\n'.join(cleaned)

def extract_content(e):
    for xpath in CONTENT_XPATHS:
        nodes = e.xpath(xpath)
        if nodes:
            raw_parts = []
            for node in nodes:
                # ä½¿ç”¨ string() æå–æ‰€æœ‰å­æ–‡æœ¬ï¼Œä¿ç•™è‡ªç„¶æ¢è¡Œ
                text = node.xpath('string(.)')
                if text:
                    raw_parts.append(text)
            raw_text = '\n'.join(raw_parts).strip()
            if len(raw_text) > 50:
                cleaned = clean_text(raw_text)
                if cleaned:
                    return cleaned
    return None

def find_next_chapter_url(e, current_url):
    # æ„å»ºå…³é”®è¯æ¡ä»¶
    text_conditions = ' or '.join([f'contains(., "{kw}")' for kw in NEXT_CHAPTER_KEYWORDS])
    # æ³¨æ„ï¼šç”¨ . è€Œä¸æ˜¯ text()ï¼Œå› ä¸ºæœ‰äº›â€œä¸‹ä¸€ç« â€åœ¨ <span> å†…
    xpath_expr = f'//a[({text_conditions}) or contains(@class, "next") or contains(@id, "next")]/@href'
    
    links = e.xpath(xpath_expr)
    links = [link.strip() for link in links if link and link.strip()]
    
    if not links:
        # å¤‡ç”¨åˆ†é¡µç­–ç•¥
        backup_xpaths = [
            '//div[@class="page"]//a[last()]/@href',
            '//div[contains(@class, "pager")]//a[last()]/@href',
            '//ul[contains(@class, "pagination")]//a[last()]/@href',
            '//a[@rel="next"]/@href',  # æ ‡å‡† rel="next"
        ]
        for xp in backup_xpaths:
            candidates = e.xpath(xp)
            if candidates:
                links = [candidates[-1].strip()]
                break

    if links:
        next_url = links[0]
        if not next_url.startswith(('http://', 'https://')):
            next_url = urljoin(current_url, next_url)
        # éªŒè¯ URL æ˜¯å¦æœ‰æ•ˆï¼ˆæœ‰ scheme å’Œ netlocï¼‰
        parsed = urlparse(next_url)
        if parsed.scheme in ('http', 'https') and parsed.netloc:
            return next_url
    return None

def get_chapter_content(url, headers, visited_urls, book_file, retry=2):
    session = requests.Session()
    for attempt in range(retry + 1):
        try:
            resp = session.get(url, headers=headers, timeout=12)
            # è‡ªåŠ¨ç¼–ç æ£€æµ‹ï¼ˆæ›´å¯é ï¼‰
            if resp.encoding == 'ISO-8859-1':
                resp.encoding = resp.apparent_encoding or 'utf-8'
            else:
                resp.encoding = 'utf-8'
            break
        except Exception as e:
            wait = 2 * (attempt + 1)
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼ˆ{url}ï¼‰ï¼Œ{wait}ç§’åç¬¬ {attempt + 1} æ¬¡é‡è¯•... é”™è¯¯: {e}")
            if attempt == retry:
                print("âŒ æœ€ç»ˆè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡æœ¬ç« ")
                return None
            time.sleep(wait)

    try:
        e = etree.HTML(resp.text)

        # è·å–æ ‡é¢˜
        title_candidates = e.xpath('//h1/text()') or e.xpath('//title/text()')
        title = title_candidates[0].strip() if title_candidates else f"æœªçŸ¥ç« èŠ‚_{int(time.time())}"
        title = re.sub(r'[\\/:*?"<>|\r\n\t]', '_', title)[:60]  # æˆªæ–­è¿‡é•¿æ ‡é¢˜

        # æå–æ­£æ–‡
        content = extract_content(e) or "ã€æ­£æ–‡æå–å¤±è´¥ã€‘\n"

        # å†™å…¥æ–‡ä»¶ï¼ˆä½¿ç”¨å½“å‰ç« èŠ‚æ•° = len(visited_urls)ï¼Œå› æœ¬ç« å·²åŠ å…¥ï¼‰
        chapter_num = len(visited_urls)
        book_file.write(f"{'='*20} ç¬¬ {chapter_num} ç« ï¼š{title} {'='*20}\n\n")
        book_file.write(content)
        book_file.write("\n\n" + "-" * 80 + "\n\n")
        book_file.flush()

        print(f"âœ… ç¬¬ {chapter_num} ç« ï¼š{title}")

        # æŸ¥æ‰¾ä¸‹ä¸€ç« 
        next_url = find_next_chapter_url(e, url)
        if next_url and next_url not in visited_urls:
            return next_url
        else:
            if next_url in visited_urls:
                print("ğŸ”„ æ£€æµ‹åˆ°å¾ªç¯é“¾æ¥ï¼Œåœæ­¢çˆ¬å–")
            else:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆä¸‹ä¸€ç« é“¾æ¥")
            return None

    except Exception as e:
        print(f"ğŸ’¥ è§£æå¼‚å¸¸ï¼ˆ{url}ï¼‰ï¼š{e}")
        return None
    finally:
        session.close()

# ===== ä¸»æµç¨‹ =====

def crawl_full_book():
    print("=" * 60)
    print("ğŸ“š å°è¯´å…¨æœ¬çˆ¬è™«ï¼ˆå¢å¼ºç¨³å®šç‰ˆ v2.1ï¼‰")
    print("=" * 60)

    # è¾“å…¥èµ·å§‹URL
    start_url = input("\nğŸ”— è¾“å…¥ç¬¬ä¸€ç« å®Œæ•´URLï¼ˆå¿…é¡»ä»¥ http(s):// å¼€å¤´ï¼‰ï¼š").strip()
    while not re.match(r'^https?://', start_url):
        start_url = input("â— æ— æ•ˆURLï¼Œè¯·é‡æ–°è¾“å…¥ï¼š").strip()

    # User-Agentï¼ˆç§»é™¤é»˜è®¤å€¼ï¼Œå¼ºåˆ¶ç”¨æˆ·è¾“å…¥ï¼‰
    print("\nâ„¹ï¸  æç¤ºï¼šUser-Agentå¯ä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„Networké¢æ¿ä¸­è·å–")
    user_agent = input("ğŸŒ è¯·è¾“å…¥ä½ çš„User-Agentï¼ˆä¸èƒ½ä¸ºç©ºï¼‰ï¼š").strip()
    # å¾ªç¯æ ¡éªŒï¼Œç›´åˆ°ç”¨æˆ·è¾“å…¥æœ‰æ•ˆå†…å®¹
    while not user_agent:
        print("âŒ é”™è¯¯ï¼šUser-Agentä¸èƒ½ä¸ºç©ºï¼Œè¯·åŠ¡å¿…è¾“å…¥ï¼")
        user_agent = input("ğŸŒ é‡æ–°è¾“å…¥User-Agentï¼š").strip()

    # ä¹¦å
    book_name = input("\nğŸ“– ä¹¦ç±åç§°ï¼ˆç”¨äºå‘½åæ–‡ä»¶å¤¹å’Œæ–‡ä»¶ï¼‰ï¼š").strip()
    if not book_name:
        domain = urlparse(start_url).netloc.replace('www.', '')
        book_name = f"{domain}_novel_{time.strftime('%Y%m%d_%H%M')}"

    # åˆ›å»ºå®‰å…¨è·¯å¾„
    safe_name = re.sub(r'[\\/:*?"<>|\r\n\t]', '_', book_name)
    book_folder = os.path.join(os.getcwd(), safe_name)
    os.makedirs(book_folder, exist_ok=True)
    book_path = os.path.join(book_folder, f"{safe_name}.txt")

    # è¯·æ±‚å¤´
    headers = {
        'User-Agent': user_agent,
        'Referer': start_url,
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Connection': 'close',
    }

    print(f"\nğŸ“‚ ä¿å­˜è·¯å¾„ï¼š{book_path}")
    print(f"â±ï¸  çˆ¬å–é—´éš”ï¼š1.5 ç§’/ç« ï¼ˆé˜²å°IPï¼‰")
    print("\nğŸš€ å¼€å§‹çˆ¬å–...\n")

    visited_urls = set()
    current_url = start_url

    with open(book_path, "w", encoding="utf-8") as f:
        while current_url and current_url not in visited_urls:
            visited_urls.add(current_url)
            next_url = get_chapter_content(current_url, headers, visited_urls, f)
            if next_url:
                time.sleep(1.5)
                current_url = next_url
            else:
                break

    print("\n" + "=" * 60)
    print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼å…± {len(visited_urls)} ç« ")
    print(f"ğŸ“„ æ–‡ä»¶ä½ç½®ï¼š{os.path.abspath(book_path)}")
    print("=" * 60)

# ===== å…¥å£ =====
if __name__ == "__main__":
    try:
        crawl_full_book()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå´©æºƒï¼š{e}")
        import traceback
        traceback.print_exc()
    finally:
        input("\næŒ‰å›è½¦é”®é€€å‡º...")


        